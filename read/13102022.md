# Reading notes
- I/O systems: https://www.cs.uic.edu/~jbell/CourseNotes/OperatingSystems/13_IOSystems.html
	- Interrupts are also used to control kernel operations, and to schedule activities for optimal performance. For example, the completion of a disk read operation involves **two** interrupts:
	    -  A high-priority interrupt acknowledges the device completion, and issues the next disk request so that the hardware does not sit idle.
	    -  A lower-priority interrupt transfers the data from the kernel memory space to the user space, and then transfers the process from the waiting queue to the ready queue.
	- Direct Memory Access
	- Blocking and Non-blocking I/O
	- Buffering
	- Kernel Data Structures
	- STREAMS
	- Performance
		- The I/O system is a major factor in overall system performance, and can place heavy loads on other major components of the system ( interrupt handling, process switching, memory access, bus contention, and CPU load for device drivers just to name a few. )
		- Interrupt handling can be relatively expensive ( slow ), which causes programmed I/O to be faster than interrupt-driven I/O when the time spent busy waiting is not excessive.
		- Network traffic can also put a heavy load on the system. Consider for example the sequence of events that occur when a single character is typed in a telnet session, as shown in figure 13.15. ( And the fact that a similar set of events must happen in reverse to echo back the character that was typed. ) Sun uses in-kernel threads for the telnet daemon, increasing the supportable number of simultaneous telnet sessions from the hundreds to the thousands.
		- Other systems use _**front-end processors**_ to off-load some of the work of I/O processing from the CPU. For example a _**terminal concentrator**_ can multiplex with hundreds of terminals on a single port on a large computer.
		- Several principles can be employed to increase the overall efficiency of I/O processing:
		    1. Reduce the number of context switches.
		    2. Reduce the number of times data must be copied.
		    3. Reduce interrupt frequency, using large transfers, buffering, and polling where appropriate.
		    4. Increase concurrency using DMA.
		    5. Move processing primitives into hardware, allowing their operation to be concurrent with CPU and bus operations.
		    6. Balance CPU, memory, bus, and I/O operations, so a bottleneck in one does not idle all the others.